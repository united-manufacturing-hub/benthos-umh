// Copyright 2025 UMH Systems GmbH
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package downsampler_plugin

import (
	"context"
	"errors"
	"fmt"
	"path/filepath"
	"reflect"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/redpanda-data/benthos/v4/public/service"
	"github.com/united-manufacturing-hub/benthos-umh/downsampler_plugin/algorithms"
)

// DefaultConfig holds the default algorithm parameters
type DefaultConfig struct {
	// Deadband parameters
	Threshold   float64       `json:"threshold,omitempty" yaml:"threshold,omitempty"`
	MaxInterval time.Duration `json:"max_interval,omitempty" yaml:"max_interval,omitempty"`

	// Swinging Door parameters
	CompDev     float64       `json:"comp_dev,omitempty" yaml:"comp_dev,omitempty"`
	CompMinTime time.Duration `json:"comp_min_time,omitempty" yaml:"comp_min_time,omitempty"`
	CompMaxTime time.Duration `json:"comp_max_time,omitempty" yaml:"comp_max_time,omitempty"`
}

// OverrideConfig defines algorithm parameter overrides for specific topics or patterns
type OverrideConfig struct {
	Pattern string `json:"pattern,omitempty" yaml:"pattern,omitempty"` // Regex pattern to match topics
	Topic   string `json:"topic,omitempty" yaml:"topic,omitempty"`     // Exact topic match

	// Deadband parameters
	Threshold   *float64       `json:"threshold,omitempty" yaml:"threshold,omitempty"`
	MaxInterval *time.Duration `json:"max_interval,omitempty" yaml:"max_interval,omitempty"`

	// Swinging Door parameters
	CompDev     *float64       `json:"comp_dev,omitempty" yaml:"comp_dev,omitempty"`
	CompMinTime *time.Duration `json:"comp_min_time,omitempty" yaml:"comp_min_time,omitempty"`
	CompMaxTime *time.Duration `json:"comp_max_time,omitempty" yaml:"comp_max_time,omitempty"`
}

// DownsamplerConfig holds the configuration for the downsampler processor
type DownsamplerConfig struct {
	Algorithm string           `json:"algorithm" yaml:"algorithm"`
	Default   DefaultConfig    `json:"default" yaml:"default"`
	Overrides []OverrideConfig `json:"overrides,omitempty" yaml:"overrides,omitempty"`
}

// SeriesState holds the state for a single time series
type SeriesState struct {
	algorithm      algorithms.DownsampleAlgorithm
	lastOutput     interface{}
	lastOutputTime time.Time
	mutex          sync.RWMutex
}

func init() {
	spec := service.NewConfigSpec().
		Version("1.0.0").
		Summary("Downsamples time-series data using configurable algorithms").
		Description(`The downsampler reduces data volume by filtering out insignificant changes in time-series data using configurable algorithms.

It processes both UMH-core time-series data and UMH classic _historian messages with data_contract "_historian", 
passing all other messages through unchanged. Each message that passes the downsampling filter is annotated 
with metadata indicating the algorithm used.

Supported formats:
- UMH-core: Single "value" field with timestamp (one tag, one message, one topic)
- UMH classic: Multiple fields in one JSON object with shared timestamp

The plugin maintains separate state for each time series (identified by umh_topic) and applies the configured algorithm
to determine whether each data point represents a significant change worth preserving.

Currently supported algorithms:
- deadband: Filters out changes smaller than a configured threshold
- swinging_door: Dynamic compression maintaining trend fidelity using Swinging Door Trending (SDT)`).
		Field(service.NewStringField("algorithm").
			Description("Downsampling algorithm to use. Supported: 'deadband', 'swinging_door'").
			Default("deadband")).
		Field(service.NewObjectField("default",
			service.NewFloatField("threshold").
				Description("Default threshold for deadband algorithm.").
				Default(0.0).
				Optional(),
			service.NewDurationField("max_interval").
				Description("Default maximum time interval for deadband algorithm.").
				Optional(),
			service.NewFloatField("comp_dev").
				Description("Default compression deviation for swinging door algorithm.").
				Default(0.5).
				Optional(),
			service.NewDurationField("comp_min_time").
				Description("Default minimum time interval for swinging door algorithm.").
				Optional(),
			service.NewDurationField("comp_max_time").
				Description("Default maximum time interval for swinging door algorithm.").
				Optional()).
			Description("Default algorithm parameters applied to all topics unless overridden.")).
		Field(service.NewObjectListField("overrides",
			service.NewStringField("pattern").
				Description("Regex pattern to match topics (e.g., '.+_counter', '^temp_'). Mutually exclusive with 'topic'.").
				Optional(),
			service.NewStringField("topic").
				Description("Exact topic to match. Mutually exclusive with 'pattern'.").
				Optional(),
			service.NewFloatField("threshold").
				Description("Override threshold for deadband algorithm.").
				Optional(),
			service.NewDurationField("max_interval").
				Description("Override maximum time interval for deadband algorithm.").
				Optional(),
			service.NewFloatField("comp_dev").
				Description("Override compression deviation for swinging door algorithm.").
				Optional(),
			service.NewDurationField("comp_min_time").
				Description("Override minimum time interval for swinging door algorithm.").
				Optional(),
			service.NewDurationField("comp_max_time").
				Description("Override maximum time interval for swinging door algorithm.").
				Optional()).
			Description("Topic-specific parameter overrides. Supports regex patterns and exact topic matching.").
			Optional())

	err := service.RegisterBatchProcessor(
		"downsampler",
		spec,
		func(conf *service.ParsedConfig, mgr *service.Resources) (service.BatchProcessor, error) {
			algorithm, err := conf.FieldString("algorithm")
			if err != nil {
				return nil, err
			}

			// Parse default configuration
			var defaultConfig DefaultConfig
			if conf.Contains("default") {
				defaultConfigParsed, err := conf.FieldObject("default")
				if err != nil {
					return nil, err
				}

				if defaultConfigParsed.Contains("threshold") {
					defaultConfig.Threshold, err = defaultConfigParsed.FieldFloat("threshold")
					if err != nil {
						return nil, fmt.Errorf("default.threshold: %w", err)
					}
				}

				if defaultConfigParsed.Contains("max_interval") {
					defaultConfig.MaxInterval, err = defaultConfigParsed.FieldDuration("max_interval")
					if err != nil {
						return nil, fmt.Errorf("default.max_interval: %w", err)
					}
				}

				if defaultConfigParsed.Contains("comp_dev") {
					defaultConfig.CompDev, err = defaultConfigParsed.FieldFloat("comp_dev")
					if err != nil {
						return nil, fmt.Errorf("default.comp_dev: %w", err)
					}
				}

				if defaultConfigParsed.Contains("comp_min_time") {
					defaultConfig.CompMinTime, err = defaultConfigParsed.FieldDuration("comp_min_time")
					if err != nil {
						return nil, fmt.Errorf("default.comp_min_time: %w", err)
					}
				}

				if defaultConfigParsed.Contains("comp_max_time") {
					defaultConfig.CompMaxTime, err = defaultConfigParsed.FieldDuration("comp_max_time")
					if err != nil {
						return nil, fmt.Errorf("default.comp_max_time: %w", err)
					}
				}
			}

			// Parse overrides configuration
			var overrides []OverrideConfig
			if conf.Contains("overrides") {
				overrideConfs, err := conf.FieldObjectList("overrides")
				if err != nil {
					return nil, err
				}

				for i, overrideConf := range overrideConfs {
					var override OverrideConfig

					// Get pattern or topic (mutually exclusive)
					hasPattern := overrideConf.Contains("pattern")
					hasTopic := overrideConf.Contains("topic")

					if hasPattern && hasTopic {
						return nil, fmt.Errorf("overrides[%d]: 'pattern' and 'topic' are mutually exclusive", i)
					}
					if !hasPattern && !hasTopic {
						return nil, fmt.Errorf("overrides[%d]: either 'pattern' or 'topic' must be specified", i)
					}

					if hasPattern {
						override.Pattern, err = overrideConf.FieldString("pattern")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].pattern: %w", i, err)
						}
					}

					if hasTopic {
						override.Topic, err = overrideConf.FieldString("topic")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].topic: %w", i, err)
						}
					}

					// Parse optional parameters using pointers
					if overrideConf.Contains("threshold") {
						threshold, err := overrideConf.FieldFloat("threshold")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].threshold: %w", i, err)
						}
						override.Threshold = &threshold
					}

					if overrideConf.Contains("max_interval") {
						maxInterval, err := overrideConf.FieldDuration("max_interval")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].max_interval: %w", i, err)
						}
						override.MaxInterval = &maxInterval
					}

					if overrideConf.Contains("comp_dev") {
						compDev, err := overrideConf.FieldFloat("comp_dev")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].comp_dev: %w", i, err)
						}
						override.CompDev = &compDev
					}

					if overrideConf.Contains("comp_min_time") {
						compMinTime, err := overrideConf.FieldDuration("comp_min_time")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].comp_min_time: %w", i, err)
						}
						override.CompMinTime = &compMinTime
					}

					if overrideConf.Contains("comp_max_time") {
						compMaxTime, err := overrideConf.FieldDuration("comp_max_time")
						if err != nil {
							return nil, fmt.Errorf("overrides[%d].comp_max_time: %w", i, err)
						}
						override.CompMaxTime = &compMaxTime
					}

					overrides = append(overrides, override)
				}
			}

			config := DownsamplerConfig{
				Algorithm: algorithm,
				Default:   defaultConfig,
				Overrides: overrides,
			}

			return newDownsamplerProcessor(config, mgr.Logger(), mgr.Metrics())
		})
	if err != nil {
		panic(err)
	}
}

// DownsamplerProcessor implements the downsampling logic
type DownsamplerProcessor struct {
	config            DownsamplerConfig
	logger            *service.Logger
	seriesState       map[string]*SeriesState
	stateMutex        sync.RWMutex
	messagesProcessed *service.MetricCounter
	messagesFiltered  *service.MetricCounter
	messagesErrored   *service.MetricCounter
	messagesPassed    *service.MetricCounter
}

func newDownsamplerProcessor(config DownsamplerConfig, logger *service.Logger, metrics *service.Metrics) (*DownsamplerProcessor, error) {
	return &DownsamplerProcessor{
		config:            config,
		logger:            logger,
		seriesState:       make(map[string]*SeriesState),
		messagesProcessed: metrics.NewCounter("messages_processed"),
		messagesFiltered:  metrics.NewCounter("messages_filtered"),
		messagesErrored:   metrics.NewCounter("messages_errored"),
		messagesPassed:    metrics.NewCounter("messages_passed_through"),
	}, nil
}

// getThresholdForTopic returns the appropriate threshold for a given topic
func (p *DownsamplerProcessor) getThresholdForTopic(topic string) float64 {
	// Check topic-specific thresholds
	for _, topicThreshold := range p.config.TopicThresholds {
		if topicThreshold.Topic != "" {
			// Exact topic match
			if topicThreshold.Topic == topic {
				return topicThreshold.Threshold
			}
		} else if topicThreshold.Pattern != "" {
			// Pattern match using filepath.Match (supports * wildcards)
			matched, err := filepath.Match(topicThreshold.Pattern, topic)
			if err != nil {
				p.logger.Warnf("Invalid pattern '%s': %v", topicThreshold.Pattern, err)
				continue
			}
			if matched {
				return topicThreshold.Threshold
			}

			// Also try matching just the last part of the topic (metric name)
			topicParts := strings.Split(topic, ".")
			if len(topicParts) > 0 {
				lastPart := topicParts[len(topicParts)-1]
				matched, err = filepath.Match(topicThreshold.Pattern, lastPart)
				if err == nil && matched {
					return topicThreshold.Threshold
				}
			}
		}
	}

	// Return default threshold if no match found
	return p.config.Threshold
}

// ProcessBatch processes a batch of messages, applying downsampling to time-series data
func (p *DownsamplerProcessor) ProcessBatch(ctx context.Context, batch service.MessageBatch) ([]service.MessageBatch, error) {
	var outBatch service.MessageBatch

	for _, msg := range batch {
		// Process both UMH-core time-series and classic _historian messages
		if !p.isTimeSeriesMessage(msg) {
			outBatch = append(outBatch, msg)
			p.messagesPassed.Incr(1)
			continue
		}

		// Parse structured payload
		data, err := msg.AsStructured()
		if err != nil {
			p.messagesErrored.Incr(1)
			p.logger.Errorf("Failed to parse structured data: %v", err)
			// Fail open - pass message through on error
			outBatch = append(outBatch, msg)
			continue
		}

		dataMap, ok := data.(map[string]interface{})
		if !ok {
			p.messagesErrored.Incr(1)
			p.logger.Errorf("Payload is not a JSON object")
			// Fail open - pass message through on error
			outBatch = append(outBatch, msg)
			continue
		}

		// Extract timestamp
		timestamp, err := p.extractTimestamp(dataMap)
		if err != nil {
			p.messagesErrored.Incr(1)
			p.logger.Errorf("Failed to extract timestamp: %v", err)
			// Fail open - pass message through on error
			outBatch = append(outBatch, msg)
			continue
		}

		// Detect message format and process accordingly
		if p.isUMHCoreFormat(dataMap) {
			// UMH-core format: single "value" field - process as before
			processedMsg, err := p.processUMHCoreMessage(msg, dataMap, timestamp)
			if err != nil {
				p.messagesErrored.Incr(1)
				p.logger.Errorf("Failed to process UMH-core message: %v", err)
				// Fail open - pass message through on error
				outBatch = append(outBatch, msg)
				continue
			}
			if processedMsg != nil {
				outBatch = append(outBatch, processedMsg)
			}
		} else {
			// UMH classic _historian format: multiple fields - process per key
			processedMsg, err := p.processUMHClassicMessage(msg, dataMap, timestamp)
			if err != nil {
				p.messagesErrored.Incr(1)
				p.logger.Errorf("Failed to process UMH classic message: %v", err)
				// Fail open - pass message through on error
				outBatch = append(outBatch, msg)
				continue
			}
			if processedMsg != nil {
				outBatch = append(outBatch, processedMsg)
			}
		}
	}

	if len(outBatch) == 0 {
		return nil, nil
	}
	return []service.MessageBatch{outBatch}, nil
}

// isTimeSeriesMessage determines if a message should be processed for downsampling
// Returns true for:
// - UMH-core time-series data (data_contract = "_historian" with structured payload)
// - UMH classic _historian data (data_contract = "_historian" with any structured payload)
func (p *DownsamplerProcessor) isTimeSeriesMessage(msg *service.Message) bool {
	contract, exists := msg.MetaGet("data_contract")
	if !exists {
		return false
	}

	// Both UMH-core and classic use _historian data contract for time-series data
	return contract == "_historian"
}

// isUMHCoreFormat checks if the message uses UMH-core format (single "value" field)
func (p *DownsamplerProcessor) isUMHCoreFormat(dataMap map[string]interface{}) bool {
	_, hasValue := dataMap["value"]
	return hasValue
}

// extractTimestamp extracts and converts timestamp from message data
func (p *DownsamplerProcessor) extractTimestamp(dataMap map[string]interface{}) (time.Time, error) {
	timestampMs, ok := dataMap["timestamp_ms"]
	if !ok {
		return time.Time{}, errors.New("missing timestamp_ms field")
	}

	var ts int64
	switch v := timestampMs.(type) {
	case float64:
		ts = int64(v)
	case int:
		ts = int64(v)
	case int64:
		ts = v
	default:
		return time.Time{}, fmt.Errorf("invalid timestamp_ms type: %T", timestampMs)
	}

	return time.Unix(0, ts*int64(time.Millisecond)), nil
}

// processUMHCoreMessage processes a UMH-core format message (single "value" field)
func (p *DownsamplerProcessor) processUMHCoreMessage(msg *service.Message, dataMap map[string]interface{}, timestamp time.Time) (*service.Message, error) {
	// Get umh_topic for series identification
	umhTopic, exists := msg.MetaGet("umh_topic")
	if !exists {
		return nil, errors.New("missing umh_topic metadata")
	}

	// Extract value
	value := dataMap["value"]
	if value == nil {
		return nil, errors.New("missing value field")
	}

	// Get or create series state
	state, err := p.getOrCreateSeriesState(umhTopic)
	if err != nil {
		return nil, fmt.Errorf("failed to get series state: %w", err)
	}

	// Apply downsampling algorithm
	shouldKeep, err := p.shouldKeepMessage(state, value, timestamp)
	if err != nil {
		return nil, fmt.Errorf("algorithm error for series %s: %w", umhTopic, err)
	}

	if shouldKeep {
		// Update state and return message
		p.updateSeriesState(state, value, timestamp)
		msg.MetaSet("downsampled_by", state.algorithm.GetMetadata())
		p.messagesProcessed.Incr(1)
		return msg, nil
	} else {
		// Message filtered
		p.messagesFiltered.Incr(1)
		return nil, nil
	}
}

// processUMHClassicMessage processes a UMH classic _historian format message (multiple fields)
func (p *DownsamplerProcessor) processUMHClassicMessage(msg *service.Message, dataMap map[string]interface{}, timestamp time.Time) (*service.Message, error) {
	// Get base topic for series identification
	baseTopic, exists := msg.MetaGet("umh_topic")
	if !exists {
		return nil, errors.New("missing umh_topic metadata")
	}

	// Create output payload with timestamp
	outputData := map[string]interface{}{
		"timestamp_ms": dataMap["timestamp_ms"],
	}

	// Track processing results
	keysProcessed := 0
	keysKept := 0
	keysDropped := 0

	// Process each field individually (excluding timestamp_ms and meta)
	for key, value := range dataMap {
		if p.isExcludedKey(key) {
			continue
		}

		keysProcessed++

		// Create series ID for this specific metric
		seriesID := p.createSeriesID(baseTopic, key)

		// Get or create series state for this metric
		state, err := p.getOrCreateSeriesState(seriesID)
		if err != nil {
			p.logger.Errorf("Failed to get series state for %s: %v", seriesID, err)
			// Fail open - keep the field
			outputData[key] = value
			keysKept++
			continue
		}

		// Apply downsampling algorithm to this field
		shouldKeep, err := p.shouldKeepMessage(state, value, timestamp)
		if err != nil {
			p.logger.Errorf("Algorithm error for series %s: %v", seriesID, err)
			// Fail open - keep the field
			outputData[key] = value
			keysKept++
			continue
		}

		if shouldKeep {
			// Keep this field
			p.updateSeriesState(state, value, timestamp)
			outputData[key] = value
			keysKept++
			p.logger.Debugf("Key kept: %s=%v in message", key, value)
		} else {
			// Drop this field
			keysDropped++
			p.logger.Debugf("Key dropped: %s=%v from message", key, value)
		}
	}

	// Log processing summary
	p.logger.Debugf("Message processed: %d keys total, %d kept, %d dropped", keysProcessed, keysKept, keysDropped)

	// Update metrics
	if keysKept > 0 {
		p.messagesProcessed.Incr(1)
	}
	if keysDropped > 0 {
		p.messagesFiltered.Incr(1)
	}

	// If no measurement fields remain (only timestamp_ms), drop the entire message
	if keysKept == 0 {
		p.logger.Debugf("Entire message dropped: no measurement fields remaining")
		return nil, nil
	}

	// Create new message with filtered data
	newMsg := msg.Copy()
	newMsg.SetStructured(outputData)

	// Add metadata annotation
	newMsg.MetaSet("downsampled_by", fmt.Sprintf("%s(filtered_%d_of_%d_keys)", p.config.Algorithm, keysKept, keysProcessed))

	return newMsg, nil
}

// isExcludedKey checks if a key should be excluded from processing
func (p *DownsamplerProcessor) isExcludedKey(key string) bool {
	excludedKeys := map[string]bool{
		"timestamp_ms": true,
		"meta":         true,
	}
	return excludedKeys[key]
}

// createSeriesID creates a unique series identifier for a specific metric within a message
func (p *DownsamplerProcessor) createSeriesID(baseTopic, metricKey string) string {
	// For classic _historian format, append the metric key to create unique series
	// Example: "umh.v1.plant1.line1._historian" + ".temperature" = "umh.v1.plant1.line1._historian.temperature"
	return baseTopic + "." + metricKey
}

// getOrCreateSeriesState retrieves or creates the state for a time series
func (p *DownsamplerProcessor) getOrCreateSeriesState(seriesID string) (*SeriesState, error) {
	p.stateMutex.RLock()
	state, exists := p.seriesState[seriesID]
	p.stateMutex.RUnlock()

	if exists {
		return state, nil
	}

	// Create new state
	p.stateMutex.Lock()
	defer p.stateMutex.Unlock()

	// Check again in case another goroutine created it
	if state, exists := p.seriesState[seriesID]; exists {
		return state, nil
	}

	// Create algorithm instance with appropriate configuration
	var algorithmConfig map[string]interface{}
	switch p.config.Algorithm {
	case "deadband":
		algorithmConfig = map[string]interface{}{
			"threshold":    p.getThresholdForTopic(seriesID),
			"max_interval": p.config.MaxInterval,
		}
	case "swinging_door":
		algorithmConfig = map[string]interface{}{
			"comp_dev":      p.config.CompDev,
			"comp_min_time": p.config.CompMinTime,
			"comp_max_time": p.config.CompMaxTime,
		}
	default:
		// Default to deadband for unknown algorithms
		algorithmConfig = map[string]interface{}{
			"threshold":    p.getThresholdForTopic(seriesID),
			"max_interval": p.config.MaxInterval,
		}
	}

	algorithm, err := algorithms.Create(p.config.Algorithm, algorithmConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create algorithm %s: %w", p.config.Algorithm, err)
	}

	state = &SeriesState{
		algorithm: algorithm,
	}

	p.seriesState[seriesID] = state
	return state, nil
}

// shouldKeepMessage determines if a message should be kept based on the algorithm
func (p *DownsamplerProcessor) shouldKeepMessage(state *SeriesState, value interface{}, timestamp time.Time) (bool, error) {
	// Handle "none" algorithm - no filtering
	if state.algorithm == nil {
		if state.lastOutput != nil {
			isEqual := p.areEqual(value, state.lastOutput)
			shouldKeep := !isEqual // Keep if values are different

			// Log dropped message if configured
			if !shouldKeep && p.logger != nil {
				p.logger.Debug(fmt.Sprintf("Dropped identical value: %v", value))
			}

			return shouldKeep, nil
		}
		return true, nil // First value always kept
	}

	// Handle boolean values with simple equality logic (never send to numeric algorithms)
	if _, isBool := value.(bool); isBool {
		if state.lastOutput != nil {
			isEqual := p.areEqual(value, state.lastOutput)
			shouldKeep := !isEqual // Keep if values are different

			// Log boolean handling
			if p.logger != nil {
				if shouldKeep {
					p.logger.Debug(fmt.Sprintf("Boolean value changed: %v -> %v (kept)", state.lastOutput, value))
				} else {
					p.logger.Debug(fmt.Sprintf("Boolean value unchanged: %v (dropped)", value))
				}
			}

			return shouldKeep, nil
		} else {
			// First boolean value is always kept
			if p.logger != nil {
				p.logger.Debug(fmt.Sprintf("First boolean value: %v (kept)", value))
			}
			return true, nil
		}
	}

	// Handle string values with simple equality logic (never send to numeric algorithms)
	if _, isString := value.(string); isString {
		if state.lastOutput != nil {
			isEqual := p.areEqual(value, state.lastOutput)
			shouldKeep := !isEqual // Keep if values are different

			// Log string handling
			if p.logger != nil {
				if shouldKeep {
					p.logger.Debug(fmt.Sprintf("String value changed: %v -> %v (kept)", state.lastOutput, value))
				} else {
					p.logger.Debug(fmt.Sprintf("String value unchanged: %v (dropped)", value))
				}
			}

			return shouldKeep, nil
		} else {
			// First string value is always kept
			if p.logger != nil {
				p.logger.Debug(fmt.Sprintf("First string value: %v (kept)", value))
			}
			return true, nil
		}
	}

	// For numeric values, use the configured algorithm
	shouldKeep, err := state.algorithm.ProcessPoint(value, timestamp)

	if err == nil && !shouldKeep && p.logger != nil {
		p.logger.Debug(fmt.Sprintf("Algorithm dropped numeric value: %v", value))
	}

	return shouldKeep, err
}

// isNonNumericType checks if a value is a non-numeric type (string, boolean, etc.)
func (p *DownsamplerProcessor) isNonNumericType(value interface{}) bool {
	switch value.(type) {
	case string, bool:
		return true
	default:
		return false
	}
}

// areEqual checks if two values are equal (used for non-numeric types)
func (p *DownsamplerProcessor) areEqual(a, b interface{}) bool {
	// Handle different types
	if reflect.TypeOf(a) != reflect.TypeOf(b) {
		return false
	}

	// Use deep equal for complex types (maps, slices, arrays)
	aType := reflect.TypeOf(a)
	if aType.Kind() == reflect.Map || aType.Kind() == reflect.Slice || aType.Kind() == reflect.Array {
		return reflect.DeepEqual(a, b)
	}

	// For simple types, direct comparison should work
	return a == b
}

// logDropReason provides detailed logging about why a message was dropped
func (p *DownsamplerProcessor) logDropReason(state *SeriesState, currentValue, previousValue interface{}, currentTime, previousTime time.Time) {
	algorithmName := state.algorithm.GetName()

	// Extract threshold for current algorithm (assuming deadband for now)
	threshold := p.extractThresholdFromMetadata(state.algorithm.GetMetadata())

	switch algorithmName {
	case "deadband":
		p.logDeadbandDropReason(currentValue, previousValue, currentTime, previousTime, threshold)
	case "swinging_door":
		p.logSwingingDoorDropReason(currentValue, previousValue, currentTime, previousTime)
	default:
		p.logger.Debugf("Message dropped by %s algorithm: current=%v, previous=%v",
			algorithmName, currentValue, previousValue)
	}
}

// logDeadbandDropReason provides specific logging for deadband algorithm drops
func (p *DownsamplerProcessor) logDeadbandDropReason(currentValue, previousValue interface{}, currentTime, previousTime time.Time, threshold float64) {
	// First message case
	if previousValue == nil {
		p.logger.Debugf("Message kept: first message in series")
		return
	}

	// Try to convert to numeric values for detailed comparison
	currentFloat, currentErr := p.toFloat64(currentValue)
	previousFloat, previousErr := p.toFloat64(previousValue)

	if currentErr != nil || previousErr != nil {
		// Non-numeric comparison
		if currentErr != nil {
			p.logger.Debugf("Message dropped: could not convert current value to numeric (%v), using equality check", currentErr)
		} else if previousErr != nil {
			p.logger.Debugf("Message dropped: could not convert previous value to numeric (%v), treating as different", previousErr)
		}
		return
	}

	// Calculate difference for numeric values
	diff := currentFloat - previousFloat
	absDiff := diff
	if absDiff < 0 {
		absDiff = -absDiff
	}

	// Time since last output
	timeSinceLastOutput := currentTime.Sub(previousTime)

	// Detailed logging with all relevant information
	p.logger.Debugf("Message dropped by deadband: current=%.6f, previous=%.6f, diff=%.6f, absDiff=%.6f, threshold=%.6f, timeSince=%v",
		currentFloat, previousFloat, diff, absDiff, threshold, timeSinceLastOutput)

	if absDiff < threshold {
		p.logger.Debugf("Drop reason: absolute difference (%.6f) below threshold (%.6f)", absDiff, threshold)
	}
}

// logSwingingDoorDropReason provides specific logging for swinging door algorithm drops
func (p *DownsamplerProcessor) logSwingingDoorDropReason(currentValue, previousValue interface{}, currentTime, previousTime time.Time) {
	// First message case
	if previousValue == nil {
		p.logger.Debugf("Message kept: first message in series")
		return
	}

	// Try to convert to numeric values for detailed comparison
	currentFloat, currentErr := p.toFloat64(currentValue)
	previousFloat, previousErr := p.toFloat64(previousValue)

	if currentErr != nil || previousErr != nil {
		// Non-numeric comparison
		if currentErr != nil {
			p.logger.Debugf("Message dropped: could not convert current value to numeric (%v), using fail-open", currentErr)
		} else if previousErr != nil {
			p.logger.Debugf("Message dropped: could not convert previous value to numeric (%v), treating as different", previousErr)
		}
		return
	}

	// Time since last output
	timeSinceLastOutput := currentTime.Sub(previousTime)

	// Detailed logging with all relevant information
	p.logger.Debugf("Message dropped by swinging_door: current=%.6f, previous=%.6f, timeSince=%v",
		currentFloat, previousFloat, timeSinceLastOutput)

	p.logger.Debugf("Drop reason: point remained within swinging door bounds")
}

// extractThresholdFromMetadata extracts threshold value from algorithm metadata string
func (p *DownsamplerProcessor) extractThresholdFromMetadata(metadata string) float64 {
	// Parse metadata string like "deadband(threshold=0.500,max_interval=30s)" or "deadband(threshold=0.500)"
	// This is a simple parser - in production you might want something more robust
	start := strings.Index(metadata, "threshold=")
	if start == -1 {
		return 0.0
	}
	start += len("threshold=")

	end := start
	for end < len(metadata) && (metadata[end] >= '0' && metadata[end] <= '9' || metadata[end] == '.') {
		end++
	}

	if end > start {
		if threshold, err := strconv.ParseFloat(metadata[start:end], 64); err == nil {
			return threshold
		}
	}

	return 0.0
}

// toFloat64 converts various numeric types to float64 (helper method)
func (p *DownsamplerProcessor) toFloat64(val interface{}) (float64, error) {
	switch v := val.(type) {
	case float64:
		return v, nil
	case float32:
		return float64(v), nil
	case int:
		return float64(v), nil
	case int8:
		return float64(v), nil
	case int16:
		return float64(v), nil
	case int32:
		return float64(v), nil
	case int64:
		return float64(v), nil
	case uint:
		return float64(v), nil
	case uint8:
		return float64(v), nil
	case uint16:
		return float64(v), nil
	case uint32:
		return float64(v), nil
	case uint64:
		return float64(v), nil
	case bool:
		if v {
			return 1.0, nil
		}
		return 0.0, nil
	case string:
		if f, err := strconv.ParseFloat(v, 64); err == nil {
			return f, nil
		}
		return 0, fmt.Errorf("cannot convert string to number: %s", v)
	default:
		return 0, fmt.Errorf("cannot convert type %T to float64", val)
	}
}

// updateSeriesState updates the state after a message is kept
func (p *DownsamplerProcessor) updateSeriesState(state *SeriesState, value interface{}, timestamp time.Time) {
	state.mutex.Lock()
	defer state.mutex.Unlock()

	state.lastOutput = value
	state.lastOutputTime = timestamp
}

// Close cleans up resources
func (p *DownsamplerProcessor) Close(ctx context.Context) error {
	// Clear all series state
	p.stateMutex.Lock()
	defer p.stateMutex.Unlock()

	for _, state := range p.seriesState {
		state.algorithm.Reset()
	}
	p.seriesState = make(map[string]*SeriesState)

	return nil
}
